# -*- coding: utf-8 -*-
"""Prompt

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rx39SHK7TUnovql_9KtGOnry3VKJQCCR
"""

# ğŸ“Œ Gerekli kÃ¼tÃ¼phanelerin yÃ¼klenmesi
!pip install transformers accelerate --quiet

# ğŸš€ Model ve tokenizer yÃ¼kleniyor
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

model_name = "google/flan-t5-large"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# ğŸ¯ Prompt tanÄ±mÄ± (Ä°ngilizce)
prompt = "Describe in detail how I will look, my profession, and lifestyle 20 years from now."

# âœï¸ GiriÅŸ verisinin hazÄ±rlanmasÄ±
inputs = tokenizer(prompt, return_tensors="pt")

# ğŸ”® Tahminin Ã¼retilmesi
output = model.generate(
    input_ids=inputs["input_ids"],
    max_new_tokens=200,
    temperature=0.7,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    repetition_penalty=1.2
)

# ğŸ–¨ï¸ Ã‡Ä±ktÄ±nÄ±n yazdÄ±rÄ±lmasÄ±
response = tokenizer.decode(output[0], skip_special_tokens=True)
print("ğŸ”® Prediction:\n", response)

# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kle (ilk defa Ã§alÄ±ÅŸtÄ±rÄ±yorsan)
!pip install diffusers transformers accelerate scipy safetensors

from diffusers import StableDiffusionPipeline
import torch

# Modeli yÃ¼kle
model_id = "runwayml/stable-diffusion-v1-5"
device = "cuda" if torch.cuda.is_available() else "cpu"

pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipe = pipe.to(device)

# GÃ¶rsel oluÅŸturmak istediÄŸin metin
prompt = ("A futuristic peaceful eco-friendly city with green skyscrapers, clean energy, "
          "a happy person living a vegan lifestyle, walking in a park, bright sunlight, "
          "highly detailed, photorealistic")

# GÃ¶rsel oluÅŸtur
image = pipe(prompt, guidance_scale=7.5).images[0]

# GÃ¶rseli gÃ¶ster
image.show()

!pip install diffusers transformers accelerate scipy safetensors

from huggingface_hub import login

hf_token = "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"  # buraya token'Ä±nÄ± yapÄ±ÅŸtÄ±r
login(token=hf_token)

from diffusers import StableDiffusionPipeline
import torch

pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16,
).to("cuda")

prompt = "a futuristic cityscape under the stars, neon lights, flying cars, cyberpunk style, ultra-detailed"
image = pipe(prompt).images[0]
image.show()

prompt = "A colorful vegetarian meal on a wooden table, fresh vegetables, clean natural lighting, top-down view, high-resolution, artistic style"

image = pipe(prompt).images[0]
image.show()  # GÃ¶rseli Jupyter/Colab iÃ§inde gÃ¶sterir

# Opsiyonel: GÃ¶rseli kaydetmek istersen
image.save("text_to_image_result.png")

from google.colab import files
files.download("text_to_image_result.png")

!pip install gradio --quiet

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from diffusers import StableDiffusionPipeline
import torch

# Model ve tokenizer yÃ¼kle (daha Ã¶nceden yaptÄ±ÄŸÄ±n gibi)
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = StableDiffusionPipeline.from_pretrained("runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16).to(device)

def generate_detailed_text(simple_prompt):
    prompt = simple_prompt + " Describe in detail how I will look, my profession, and lifestyle 20 years from now."
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        max_new_tokens=200,
        temperature=0.7,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        repetition_penalty=1.2
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

def generate_image_from_text(detailed_prompt):
    image = pipe(detailed_prompt, guidance_scale=7.5).images[0]
    return image

import gradio as gr

def generate_all(prompt):
    detailed_text = generate_detailed_text(prompt)
    image = generate_image_from_text(detailed_text)
    return image, detailed_text

iface = gr.Interface(
    fn=generate_all,
    inputs=gr.Textbox(lines=2, placeholder="Basit tanÄ±mÄ± yazÄ±nÄ±z..."),
    outputs=[gr.Image(type="pil"), gr.Textbox(label="DetaylandÄ±rÄ±lmÄ±ÅŸ AÃ§Ä±klama")],
    title="AkÄ±llÄ± Metinden GÃ¶rsele DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼",
    description="Basit bir metin girin, 20 yÄ±l sonraki detaylÄ± aÃ§Ä±klama ve gÃ¶rselini oluÅŸturun."
)

iface.launch()

!pip install gradio transformers diffusers accelerate scipy safetensors --quiet

import gradio as gr
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from diffusers import StableDiffusionPipeline
import torch

device = "cuda" if torch.cuda.is_available() else "cpu"

# Dil modeli
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)

# GÃ¶rsel modeli
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5",
    torch_dtype=torch.float16 if device=="cuda" else torch.float32,
).to(device)

def generate_detailed_text(simple_prompt):
    prompt = simple_prompt + " Describe in detail how I will look, my profession, and lifestyle 20 years from now."
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    outputs = model.generate(
        input_ids=inputs["input_ids"],
        max_new_tokens=100,
        temperature=0.8,
        do_sample=True,
        top_k=30,
        top_p=0.9,
        repetition_penalty=1.1
    )
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

def generate_image_from_text(detailed_prompt):
    image = pipe(detailed_prompt, guidance_scale=7.5).images[0]
    return image

def generate_all(prompt):
    try:
        detailed_text = generate_detailed_text(prompt)
    except Exception as e:
        return None, f"Dil modeli hatasÄ±: {str(e)}"

    try:
        image = generate_image_from_text(detailed_text)
    except Exception as e:
        return None, f"GÃ¶rsel modeli hatasÄ±: {str(e)}"

    return image, detailed_text

iface = gr.Interface(
    fn=generate_all,
    inputs=gr.Textbox(lines=2, placeholder="Basit tanÄ±mÄ± yazÄ±nÄ±z..."),
    outputs=[gr.Image(type="pil"), gr.Textbox(label="DetaylandÄ±rÄ±lmÄ±ÅŸ AÃ§Ä±klama")],
    title="AkÄ±llÄ± Metinden GÃ¶rsele DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼",
    description="Basit bir metin girin, 20 yÄ±l sonraki detaylÄ± aÃ§Ä±klama ve gÃ¶rselini oluÅŸturun."
)

iface.launch()

import gradio as gr
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from diffusers import StableDiffusionPipeline
import torch

# --- Dil modeli yÃ¼kle ---
model_name = "google/flan-t5-large"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# --- GÃ¶rsel modeli yÃ¼kle ---
device = "cuda" if torch.cuda.is_available() else "cpu"
pipe = StableDiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16 if device=="cuda" else torch.float32
).to(device)

# --- Dil modelinden detaylÄ± metin Ã¼reten fonksiyon ---
def generate_detailed_text(prompt):
    expanded_prompt = f"Describe in detail how this scene will look 20 years from now: {prompt}"
    inputs = tokenizer(expanded_prompt, return_tensors="pt")
    output = model.generate(
        input_ids=inputs["input_ids"],
        max_new_tokens=200,
        temperature=0.7,
        do_sample=True,
        top_k=50,
        top_p=0.95,
        repetition_penalty=1.2
    )
    detailed_text = tokenizer.decode(output[0], skip_special_tokens=True)
    return detailed_text

# --- GÃ¶rsel modeliyle gÃ¶rsel Ã¼reten fonksiyon ---
def generate_image_from_text(detailed_prompt):
    image = pipe(detailed_prompt, guidance_scale=7.5).images[0]
    return image

# --- Ana fonksiyon: Ã¶nce metni detaylandÄ±r, sonra gÃ¶rseli oluÅŸtur ---
def generate_all(prompt):
    try:
        detailed_text = generate_detailed_text(prompt)
    except Exception as e:
        return None, f"Dil modeli hatasÄ±: {str(e)}"

    try:
        image = generate_image_from_text(detailed_text)
    except Exception as e:
        return None, f"GÃ¶rsel modeli hatasÄ±: {str(e)}"

    return image, detailed_text

# --- Gradio arayÃ¼zÃ¼ ---
iface = gr.Interface(
    fn=generate_all,
    inputs=gr.Textbox(lines=2, placeholder="Basit tanÄ±mÄ± yazÄ±nÄ±z..."),
    outputs=[gr.Image(type="pil"), gr.Textbox(label="DetaylandÄ±rÄ±lmÄ±ÅŸ AÃ§Ä±klama")],
    title="AkÄ±llÄ± Metinden GÃ¶rsele DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼",
    description="Basit bir metin girin, 20 yÄ±l sonraki detaylÄ± aÃ§Ä±klama ve gÃ¶rselini oluÅŸturur."
)

iface.launch()

!pip install --upgrade gradio

import gradio as gr

def process_audio(audio):
    if audio is None:
        return "HiÃ§ ses kaydedilmedi."
    return "Ses baÅŸarÄ±yla alÄ±ndÄ±!"

iface = gr.Interface(
    fn=process_audio,
    inputs=gr.Audio(),  # source parametresi yok
    outputs="text"
)

iface.launch()